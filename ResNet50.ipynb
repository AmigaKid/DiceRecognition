{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Add, Dense, Activation, Dropout, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "import os\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(labellist, nb_classes):\n",
    "    one_hot = np.zeros([len(labellist), nb_classes])\n",
    "    for i,l in enumerate(labellist):\n",
    "        one_hot[i][int(l)-1] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = Conv2D(filters = F1, kernel_size=(1,1), strides=(1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(filters = F2, kernel_size=(f,f), strides=(1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(filters = F3, kernel_size=(1,1), strides=(1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = Conv2D(F1, (1,1), strides = (s,s), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F2, (f,f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(F3, (1,1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name = bn_name_base + '2c')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(F3, (1,1), strides=(s,s), padding='valid', name = conv_name_base+ '1', kernel_initializer= glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "    \n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dropout(rate=0.2)(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape= (64,64,3), classes=6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    #Stage 1\n",
    "    X = Conv2D(64, (7,7), strides = (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3,3), strides=(2,2))(X)\n",
    "    #X = Dropout(rate=0.2)(X)\n",
    "    \n",
    "    #Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
    "    X = identity_block(X, 3, [64,64, 256], stage = 2, block = 'b')\n",
    "    X = identity_block(X, 3, [64,64, 256], stage = 2, block = 'c')\n",
    "    \n",
    "    #Stage 3\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage = 3, block = 'b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage = 3, block = 'c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage = 3, block = 'd')\n",
    "    \n",
    "    #Stage 4\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage = 4, block = 'b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage = 4, block = 'c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage = 4, block = 'd')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage = 4, block = 'e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage = 4, block = 'f')\n",
    "        \n",
    "    #Stage 5\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage = 5, block = 'b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage = 5, block = 'c')\n",
    "    \n",
    "    X = AveragePooling2D(pool_size=(2,2), strides = None, padding='valid', data_format=None)(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer= glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNets50')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftraindir = r'C:\\Users\\dhecker\\Downloads\\dice\\train'\n",
    "ftestdir = r'C:\\Users\\dhecker\\Downloads\\dice\\valid'\n",
    "IMSIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 12337 images and set 12337 labels\n"
     ]
    }
   ],
   "source": [
    "#load training data\n",
    "dirs = ['d6','d8','d10','d12','d20']\n",
    "loaded_images = list()\n",
    "labels = []\n",
    "for d in dirs:\n",
    "    files = os.listdir(os.path.join(ftraindir,d))\n",
    "    for f in files:\n",
    "        im = image.imread(os.path.join(ftraindir,d,f))\n",
    "        im = np.resize(im, (IMSIZE,IMSIZE,3))\n",
    "        loaded_images.append(im) #images as LIST\n",
    "        \n",
    "        #print('> loaded %s %s' % (f, im.shape))\n",
    "        \n",
    "        #set label\n",
    "        if f[0:2] == 'd4':\n",
    "            labels.append(1)\n",
    "        elif f[0:2] == 'd6':\n",
    "            labels.append(2)\n",
    "        elif f[0:2] == 'd8':\n",
    "            labels.append(3)\n",
    "        elif f[0:3] == 'd10':\n",
    "            labels.append(4)\n",
    "        elif f[0:3] == 'd12':\n",
    "            labels.append(5)\n",
    "        elif f[0:3] == 'd20':\n",
    "            labels.append(6)\n",
    "            \n",
    "X_train_orig = np.array(loaded_images)\n",
    "Y_train_orig = labels\n",
    "print('loaded %i images and set %i labels' % (X_train_orig.shape[0], len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1761 images and set 1761 labels\n"
     ]
    }
   ],
   "source": [
    "#load test data\n",
    "loaded_images = list()\n",
    "labels = []\n",
    "for d in dirs:\n",
    "    files = os.listdir(os.path.join(ftestdir,d))\n",
    "    for f in files:\n",
    "        if f[0] == 'd':\n",
    "            \n",
    "            im = image.imread(os.path.join(ftestdir,d,f))\n",
    "            im = np.resize(im, (IMSIZE,IMSIZE,3))\n",
    "            loaded_images.append(im)\n",
    "        \n",
    "            #set label\n",
    "            if f[0:2] == 'd4':\n",
    "                labels.append(1)\n",
    "            elif f[0:2] == 'd6':\n",
    "                labels.append(2)\n",
    "            elif f[0:2] == 'd8':\n",
    "                labels.append(3)\n",
    "            elif f[0:3] == 'd10':\n",
    "                labels.append(4)\n",
    "            elif f[0:3] == 'd12':\n",
    "                labels.append(5)\n",
    "            elif f[0:3] == 'd20':\n",
    "                labels.append(6)\n",
    "            \n",
    "X_test_orig = np.array(loaded_images)\n",
    "Y_test_orig = labels\n",
    "print('loaded %i images and set %i labels' % (X_test_orig.shape[0], len(Y_test_orig)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 12337\n",
      "number of test examples = 1761\n",
      "X_train shape: (12337, 64, 64, 3)\n",
      "Y_train shape: (12337, 6)\n",
      "X_test.shape: (1761, 64, 64, 3)\n",
      "Y_test.shape: (1761, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = one_hot_encoder(Y_train_orig, 6)\n",
    "Y_test = one_hot_encoder(Y_test_orig, 6)\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape)) \n",
    "print(\"X_test.shape: \" + str(X_test.shape))\n",
    "print(\"Y_test.shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "12337/12337 [==============================] - 125s 10ms/step - loss: 1.9483 - acc: 0.2856\n",
      "Epoch 2/250\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.6798 - acc: 0.3304\n",
      "Epoch 3/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 1.5771 - acc: 0.3375\n",
      "Epoch 4/250\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.5271 - acc: 0.3552\n",
      "Epoch 5/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 1.4905 - acc: 0.3584\n",
      "Epoch 6/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 1.4440 - acc: 0.3691\n",
      "Epoch 7/250\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.4289 - acc: 0.3744\n",
      "Epoch 8/250\n",
      "12337/12337 [==============================] - 384s 31ms/step - loss: 1.4050 - acc: 0.3828\n",
      "Epoch 9/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 1.4145 - acc: 0.3782\n",
      "Epoch 10/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 1.3571 - acc: 0.3961\n",
      "Epoch 11/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 1.3547 - acc: 0.3972\n",
      "Epoch 12/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 1.3264 - acc: 0.4057\n",
      "Epoch 13/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 1.3187 - acc: 0.4108\n",
      "Epoch 14/250\n",
      "12337/12337 [==============================] - 348s 28ms/step - loss: 1.2884 - acc: 0.4251\n",
      "Epoch 15/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 1.2622 - acc: 0.4357\n",
      "Epoch 16/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 1.2239 - acc: 0.4515\n",
      "Epoch 17/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 1.2066 - acc: 0.4592\n",
      "Epoch 18/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 1.1828 - acc: 0.4731\n",
      "Epoch 19/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 1.1611 - acc: 0.4839\n",
      "Epoch 20/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 1.1296 - acc: 0.4880\n",
      "Epoch 21/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 1.0907 - acc: 0.5011\n",
      "Epoch 22/250\n",
      "12337/12337 [==============================] - 349s 28ms/step - loss: 1.0647 - acc: 0.5109\n",
      "Epoch 23/250\n",
      "12337/12337 [==============================] - 349s 28ms/step - loss: 1.0312 - acc: 0.5228\n",
      "Epoch 24/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 1.0091 - acc: 0.5382\n",
      "Epoch 25/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.9975 - acc: 0.5384\n",
      "Epoch 26/250\n",
      "12337/12337 [==============================] - 352s 28ms/step - loss: 0.9733 - acc: 0.5462\n",
      "Epoch 27/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.9486 - acc: 0.5532\n",
      "Epoch 28/250\n",
      "12337/12337 [==============================] - 349s 28ms/step - loss: 0.9331 - acc: 0.5643\n",
      "Epoch 29/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.9225 - acc: 0.5674\n",
      "Epoch 30/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.9104 - acc: 0.5765\n",
      "Epoch 31/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.8959 - acc: 0.5864\n",
      "Epoch 32/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.8944 - acc: 0.5865\n",
      "Epoch 33/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.8676 - acc: 0.6006\n",
      "Epoch 34/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.8513 - acc: 0.6076\n",
      "Epoch 35/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 0.8569 - acc: 0.6072\n",
      "Epoch 36/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 0.8270 - acc: 0.6251\n",
      "Epoch 37/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.8219 - acc: 0.6276\n",
      "Epoch 38/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.8091 - acc: 0.6338\n",
      "Epoch 39/250\n",
      "12337/12337 [==============================] - 361s 29ms/step - loss: 0.7902 - acc: 0.6487\n",
      "Epoch 40/250\n",
      "12337/12337 [==============================] - 365s 30ms/step - loss: 0.7857 - acc: 0.6496\n",
      "Epoch 41/250\n",
      "12337/12337 [==============================] - 365s 30ms/step - loss: 0.7732 - acc: 0.6542\n",
      "Epoch 42/250\n",
      "12337/12337 [==============================] - 368s 30ms/step - loss: 0.7374 - acc: 0.6782\n",
      "Epoch 43/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.7339 - acc: 0.6717\n",
      "Epoch 44/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.7325 - acc: 0.6776\n",
      "Epoch 45/250\n",
      "12337/12337 [==============================] - 360s 29ms/step - loss: 0.7187 - acc: 0.6803\n",
      "Epoch 46/250\n",
      "12337/12337 [==============================] - 362s 29ms/step - loss: 0.6998 - acc: 0.6902\n",
      "Epoch 47/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.6934 - acc: 0.7039\n",
      "Epoch 48/250\n",
      "12337/12337 [==============================] - 316s 26ms/step - loss: 0.6722 - acc: 0.7027\n",
      "Epoch 49/250\n",
      "12337/12337 [==============================] - 312s 25ms/step - loss: 0.6538 - acc: 0.7178\n",
      "Epoch 50/250\n",
      "12337/12337 [==============================] - 311s 25ms/step - loss: 0.6466 - acc: 0.7225\n",
      "Epoch 51/250\n",
      "12337/12337 [==============================] - 312s 25ms/step - loss: 0.6455 - acc: 0.7215\n",
      "Epoch 52/250\n",
      "12337/12337 [==============================] - 334s 27ms/step - loss: 0.6313 - acc: 0.7319\n",
      "Epoch 53/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.6228 - acc: 0.7302\n",
      "Epoch 54/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.5981 - acc: 0.7440\n",
      "Epoch 55/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5956 - acc: 0.7446\n",
      "Epoch 56/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5794 - acc: 0.7515\n",
      "Epoch 57/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5778 - acc: 0.7519\n",
      "Epoch 58/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5840 - acc: 0.7525\n",
      "Epoch 59/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.5752 - acc: 0.7567\n",
      "Epoch 60/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.5592 - acc: 0.7621\n",
      "Epoch 61/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.5522 - acc: 0.7649\n",
      "Epoch 62/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5350 - acc: 0.7755\n",
      "Epoch 63/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5312 - acc: 0.7757\n",
      "Epoch 64/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.5378 - acc: 0.7769\n",
      "Epoch 65/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.5052 - acc: 0.7854\n",
      "Epoch 66/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.5159 - acc: 0.7863\n",
      "Epoch 67/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.5038 - acc: 0.7865\n",
      "Epoch 68/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.4987 - acc: 0.7893\n",
      "Epoch 69/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.4962 - acc: 0.7881\n",
      "Epoch 70/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4814 - acc: 0.7951\n",
      "Epoch 71/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4772 - acc: 0.7988\n",
      "Epoch 72/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4763 - acc: 0.7987\n",
      "Epoch 73/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.4570 - acc: 0.8074\n",
      "Epoch 74/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.4677 - acc: 0.8016\n",
      "Epoch 75/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4603 - acc: 0.8082\n",
      "Epoch 76/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4514 - acc: 0.8074\n",
      "Epoch 77/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4421 - acc: 0.8112\n",
      "Epoch 78/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.4369 - acc: 0.8142\n",
      "Epoch 79/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.4441 - acc: 0.8119\n",
      "Epoch 80/250\n",
      "12337/12337 [==============================] - 327s 26ms/step - loss: 0.4365 - acc: 0.8173\n",
      "Epoch 81/250\n",
      "12337/12337 [==============================] - 312s 25ms/step - loss: 0.4214 - acc: 0.8231\n",
      "Epoch 82/250\n",
      "12337/12337 [==============================] - 310s 25ms/step - loss: 0.4152 - acc: 0.8289\n",
      "Epoch 83/250\n",
      "12337/12337 [==============================] - 311s 25ms/step - loss: 0.4171 - acc: 0.8241\n",
      "Epoch 84/250\n",
      "12337/12337 [==============================] - 324s 26ms/step - loss: 0.4096 - acc: 0.8291\n",
      "Epoch 85/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.4076 - acc: 0.8296\n",
      "Epoch 86/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.4158 - acc: 0.8292\n",
      "Epoch 87/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.4011 - acc: 0.8290\n",
      "Epoch 88/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.4059 - acc: 0.8269\n",
      "Epoch 89/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.4029 - acc: 0.8339\n",
      "Epoch 90/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.4032 - acc: 0.8319\n",
      "Epoch 91/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3883 - acc: 0.8369\n",
      "Epoch 92/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.3809 - acc: 0.8402\n",
      "Epoch 93/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 0.3808 - acc: 0.8363\n",
      "Epoch 94/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.3755 - acc: 0.8416\n",
      "Epoch 95/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3740 - acc: 0.8415\n",
      "Epoch 96/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3710 - acc: 0.8426\n",
      "Epoch 97/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3692 - acc: 0.8454\n",
      "Epoch 98/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3730 - acc: 0.8420\n",
      "Epoch 99/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3636 - acc: 0.8466\n",
      "Epoch 100/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3680 - acc: 0.8432\n",
      "Epoch 101/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3555 - acc: 0.8492\n",
      "Epoch 102/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3480 - acc: 0.8517\n",
      "Epoch 103/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.3560 - acc: 0.8463\n",
      "Epoch 104/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3475 - acc: 0.8503\n",
      "Epoch 105/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3448 - acc: 0.8540\n",
      "Epoch 106/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3419 - acc: 0.8554\n",
      "Epoch 107/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3438 - acc: 0.8517\n",
      "Epoch 108/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.3409 - acc: 0.8544\n",
      "Epoch 109/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.3486 - acc: 0.8496\n",
      "Epoch 110/250\n",
      "12337/12337 [==============================] - 359s 29ms/step - loss: 0.3394 - acc: 0.8573\n",
      "Epoch 111/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3342 - acc: 0.8599\n",
      "Epoch 112/250\n",
      "12337/12337 [==============================] - 352s 28ms/step - loss: 0.3302 - acc: 0.8605\n",
      "Epoch 113/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3300 - acc: 0.8576\n",
      "Epoch 114/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.3318 - acc: 0.8601\n",
      "Epoch 115/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3205 - acc: 0.8650\n",
      "Epoch 116/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3277 - acc: 0.8607\n",
      "Epoch 117/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.3208 - acc: 0.8665\n",
      "Epoch 118/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.3150 - acc: 0.8644\n",
      "Epoch 119/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3157 - acc: 0.8687\n",
      "Epoch 120/250\n",
      "12337/12337 [==============================] - 324s 26ms/step - loss: 0.3094 - acc: 0.8692\n",
      "Epoch 121/250\n",
      "12337/12337 [==============================] - 313s 25ms/step - loss: 0.3062 - acc: 0.8693\n",
      "Epoch 122/250\n",
      "12337/12337 [==============================] - 314s 25ms/step - loss: 0.3200 - acc: 0.8678\n",
      "Epoch 123/250\n",
      "12337/12337 [==============================] - 314s 25ms/step - loss: 0.3101 - acc: 0.8648\n",
      "Epoch 124/250\n",
      "12337/12337 [==============================] - 333s 27ms/step - loss: 0.3085 - acc: 0.8690\n",
      "Epoch 125/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.3003 - acc: 0.8727\n",
      "Epoch 126/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.3163 - acc: 0.8638\n",
      "Epoch 127/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2907 - acc: 0.8729\n",
      "Epoch 128/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2986 - acc: 0.8729\n",
      "Epoch 129/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2921 - acc: 0.8762\n",
      "Epoch 130/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.2963 - acc: 0.8766\n",
      "Epoch 131/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.3023 - acc: 0.8704\n",
      "Epoch 132/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2937 - acc: 0.8769\n",
      "Epoch 133/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2978 - acc: 0.8756\n",
      "Epoch 134/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.2835 - acc: 0.8795\n",
      "Epoch 135/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.2937 - acc: 0.8761\n",
      "Epoch 136/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.2926 - acc: 0.8776\n",
      "Epoch 137/250\n",
      "12337/12337 [==============================] - 334s 27ms/step - loss: 0.2902 - acc: 0.8780\n",
      "Epoch 138/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.2754 - acc: 0.8803\n",
      "Epoch 139/250\n",
      "12337/12337 [==============================] - 359s 29ms/step - loss: 0.2657 - acc: 0.8868\n",
      "Epoch 140/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.2841 - acc: 0.8784\n",
      "Epoch 141/250\n",
      "12337/12337 [==============================] - 359s 29ms/step - loss: 0.2849 - acc: 0.8807\n",
      "Epoch 142/250\n",
      "12337/12337 [==============================] - 360s 29ms/step - loss: 0.2689 - acc: 0.8866\n",
      "Epoch 143/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.2796 - acc: 0.8808\n",
      "Epoch 144/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.2644 - acc: 0.8885\n",
      "Epoch 145/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2833 - acc: 0.8818\n",
      "Epoch 146/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2689 - acc: 0.8872\n",
      "Epoch 147/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2705 - acc: 0.8875\n",
      "Epoch 148/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2648 - acc: 0.8860\n",
      "Epoch 149/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.2637 - acc: 0.8884\n",
      "Epoch 150/250\n",
      "12337/12337 [==============================] - 350s 28ms/step - loss: 0.2684 - acc: 0.8875\n",
      "Epoch 151/250\n",
      "12337/12337 [==============================] - 351s 28ms/step - loss: 0.2619 - acc: 0.8924\n",
      "Epoch 152/250\n",
      "12337/12337 [==============================] - 341s 28ms/step - loss: 0.2628 - acc: 0.8885\n",
      "Epoch 153/250\n",
      "12337/12337 [==============================] - 313s 25ms/step - loss: 0.2577 - acc: 0.8890\n",
      "Epoch 154/250\n",
      "12337/12337 [==============================] - 313s 25ms/step - loss: 0.2504 - acc: 0.8944\n",
      "Epoch 155/250\n",
      "12337/12337 [==============================] - 315s 26ms/step - loss: 0.2616 - acc: 0.8885\n",
      "Epoch 156/250\n",
      "12337/12337 [==============================] - 328s 27ms/step - loss: 0.2498 - acc: 0.8927\n",
      "Epoch 157/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2541 - acc: 0.8923\n",
      "Epoch 158/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2518 - acc: 0.8971\n",
      "Epoch 159/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2540 - acc: 0.8914\n",
      "Epoch 160/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2633 - acc: 0.8902\n",
      "Epoch 161/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.2502 - acc: 0.8961\n",
      "Epoch 162/250\n",
      "12337/12337 [==============================] - 357s 29ms/step - loss: 0.2404 - acc: 0.8983\n",
      "Epoch 163/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2506 - acc: 0.8950\n",
      "Epoch 164/250\n",
      "12337/12337 [==============================] - 356s 29ms/step - loss: 0.2435 - acc: 0.8984\n",
      "Epoch 165/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2436 - acc: 0.8987\n",
      "Epoch 166/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.2518 - acc: 0.8939\n",
      "Epoch 167/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.2387 - acc: 0.8995\n",
      "Epoch 168/250\n",
      "12337/12337 [==============================] - 355s 29ms/step - loss: 0.2416 - acc: 0.8963\n",
      "Epoch 169/250\n",
      "12337/12337 [==============================] - 354s 29ms/step - loss: 0.2427 - acc: 0.8971\n",
      "Epoch 170/250\n",
      "12337/12337 [==============================] - 353s 29ms/step - loss: 0.2383 - acc: 0.8999\n",
      "Epoch 171/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.2350 - acc: 0.9000\n",
      "Epoch 172/250\n",
      "12337/12337 [==============================] - 352s 29ms/step - loss: 0.2428 - acc: 0.8951\n",
      "Epoch 173/250\n",
      "12337/12337 [==============================] - 358s 29ms/step - loss: 0.2277 - acc: 0.9017\n",
      "Epoch 174/250\n",
      "12337/12337 [==============================] - 276s 22ms/step - loss: 0.2350 - acc: 0.8991\n",
      "Epoch 175/250\n",
      "12337/12337 [==============================] - 183s 15ms/step - loss: 0.2329 - acc: 0.9044\n",
      "Epoch 176/250\n",
      "12337/12337 [==============================] - 179s 14ms/step - loss: 0.2366 - acc: 0.9016\n",
      "Epoch 177/250\n",
      "12337/12337 [==============================] - 179s 14ms/step - loss: 0.2251 - acc: 0.9054\n",
      "Epoch 178/250\n",
      "12337/12337 [==============================] - 179s 14ms/step - loss: 0.2259 - acc: 0.9039\n",
      "Epoch 179/250\n",
      "12337/12337 [==============================] - 179s 15ms/step - loss: 0.2332 - acc: 0.9036\n",
      "Epoch 180/250\n",
      "12337/12337 [==============================] - 179s 15ms/step - loss: 0.2164 - acc: 0.9069\n",
      "Epoch 181/250\n",
      "12337/12337 [==============================] - 180s 15ms/step - loss: 0.2304 - acc: 0.9043\n",
      "Epoch 182/250\n",
      "12337/12337 [==============================] - 179s 15ms/step - loss: 0.2191 - acc: 0.9097\n",
      "Epoch 183/250\n",
      "12337/12337 [==============================] - 179s 15ms/step - loss: 0.2224 - acc: 0.9052\n",
      "Epoch 184/250\n",
      "12337/12337 [==============================] - 177s 14ms/step - loss: 0.2280 - acc: 0.9016\n",
      "Epoch 185/250\n",
      "12337/12337 [==============================] - 178s 14ms/step - loss: 0.2179 - acc: 0.9102\n",
      "Epoch 186/250\n",
      "12337/12337 [==============================] - 178s 14ms/step - loss: 0.2232 - acc: 0.9061\n",
      "Epoch 187/250\n",
      "12337/12337 [==============================] - 178s 14ms/step - loss: 0.2267 - acc: 0.9047\n",
      "Epoch 188/250\n",
      "12337/12337 [==============================] - 182s 15ms/step - loss: 0.2168 - acc: 0.9088\n",
      "Epoch 189/250\n",
      "12337/12337 [==============================] - 195s 16ms/step - loss: 0.2141 - acc: 0.9101\n",
      "Epoch 190/250\n",
      "12337/12337 [==============================] - 180s 15ms/step - loss: 0.2157 - acc: 0.9068\n",
      "Epoch 191/250\n",
      "12337/12337 [==============================] - 179s 15ms/step - loss: 0.2186 - acc: 0.9064\n",
      "Epoch 192/250\n",
      "12337/12337 [==============================] - 199s 16ms/step - loss: 0.2114 - acc: 0.9104\n",
      "Epoch 193/250\n",
      "12337/12337 [==============================] - 223s 18ms/step - loss: 0.2138 - acc: 0.9085\n",
      "Epoch 194/250\n",
      "12337/12337 [==============================] - 241s 20ms/step - loss: 0.2186 - acc: 0.9060\n",
      "Epoch 195/250\n",
      "12337/12337 [==============================] - 186s 15ms/step - loss: 0.2170 - acc: 0.9078\n",
      "Epoch 196/250\n",
      "12337/12337 [==============================] - 120s 10ms/step - loss: 0.2188 - acc: 0.9069\n",
      "Epoch 197/250\n",
      "12337/12337 [==============================] - 111s 9ms/step - loss: 0.2068 - acc: 0.9134\n",
      "Epoch 198/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.2107 - acc: 0.9096\n",
      "Epoch 199/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2078 - acc: 0.9134\n",
      "Epoch 200/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2110 - acc: 0.9113\n",
      "Epoch 201/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2006 - acc: 0.9166\n",
      "Epoch 202/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2082 - acc: 0.9129\n",
      "Epoch 203/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2105 - acc: 0.9134\n",
      "Epoch 204/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2058 - acc: 0.9132\n",
      "Epoch 205/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2078 - acc: 0.9130\n",
      "Epoch 206/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2082 - acc: 0.9127\n",
      "Epoch 207/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1950 - acc: 0.9128\n",
      "Epoch 208/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2047 - acc: 0.9142\n",
      "Epoch 209/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2004 - acc: 0.9159\n",
      "Epoch 210/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2017 - acc: 0.9132\n",
      "Epoch 211/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.2010 - acc: 0.9134\n",
      "Epoch 212/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.2006 - acc: 0.9146\n",
      "Epoch 213/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1983 - acc: 0.9156\n",
      "Epoch 214/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1953 - acc: 0.9186\n",
      "Epoch 215/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1968 - acc: 0.9186\n",
      "Epoch 216/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1913 - acc: 0.9193\n",
      "Epoch 217/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1947 - acc: 0.9177\n",
      "Epoch 218/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.2012 - acc: 0.9163\n",
      "Epoch 219/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1938 - acc: 0.9188\n",
      "Epoch 220/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1928 - acc: 0.9155\n",
      "Epoch 221/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1964 - acc: 0.9150\n",
      "Epoch 222/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1913 - acc: 0.9200\n",
      "Epoch 223/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1940 - acc: 0.9201\n",
      "Epoch 224/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1974 - acc: 0.9187\n",
      "Epoch 225/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1876 - acc: 0.9199\n",
      "Epoch 226/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1886 - acc: 0.9190\n",
      "Epoch 227/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1862 - acc: 0.9183\n",
      "Epoch 228/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1882 - acc: 0.9219\n",
      "Epoch 229/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1899 - acc: 0.9205\n",
      "Epoch 230/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1907 - acc: 0.9195\n",
      "Epoch 231/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1793 - acc: 0.9253\n",
      "Epoch 232/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1883 - acc: 0.9200\n",
      "Epoch 233/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1900 - acc: 0.9216\n",
      "Epoch 234/250\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1823 - acc: 0.9272\n",
      "Epoch 235/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1952 - acc: 0.9172\n",
      "Epoch 236/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1867 - acc: 0.9213\n",
      "Epoch 237/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1757 - acc: 0.9254\n",
      "Epoch 238/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1818 - acc: 0.9224\n",
      "Epoch 239/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1737 - acc: 0.9276\n",
      "Epoch 240/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1804 - acc: 0.9241\n",
      "Epoch 241/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1796 - acc: 0.9221\n",
      "Epoch 242/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1748 - acc: 0.9249\n",
      "Epoch 243/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1862 - acc: 0.9238\n",
      "Epoch 244/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1815 - acc: 0.9236\n",
      "Epoch 245/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1757 - acc: 0.9275\n",
      "Epoch 246/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1820 - acc: 0.9232\n",
      "Epoch 247/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1744 - acc: 0.9262\n",
      "Epoch 248/250\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1733 - acc: 0.9266\n",
      "Epoch 249/250\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1819 - acc: 0.9245\n",
      "Epoch 250/250\n",
      "12337/12337 [==============================] - 111s 9ms/step - loss: 0.1757 - acc: 0.9227\n",
      "1761/1761 [==============================] - 8s 4ms/step\n",
      "Loss = 1.881839858909063\n",
      "Test Accuracy = 0.5474162407722885\n"
     ]
    }
   ],
   "source": [
    "#20% dropout at the end of every ident and conv block\n",
    "model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "adam = optimizers.Adam(0.00001)#, 0.9, 0.999, False)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=250, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2404 - acc: 0.8976\n",
      "Epoch 2/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2507 - acc: 0.8969\n",
      "Epoch 3/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2373 - acc: 0.9031\n",
      "Epoch 4/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2516 - acc: 0.9006\n",
      "Epoch 5/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2418 - acc: 0.8991\n",
      "Epoch 6/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2278 - acc: 0.9072\n",
      "Epoch 7/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2219 - acc: 0.9057\n",
      "Epoch 8/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2180 - acc: 0.9101\n",
      "Epoch 9/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2249 - acc: 0.9081\n",
      "Epoch 10/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2090 - acc: 0.9133\n",
      "Epoch 11/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2515 - acc: 0.9009\n",
      "Epoch 12/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2111 - acc: 0.9139\n",
      "Epoch 13/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2142 - acc: 0.9104\n",
      "Epoch 14/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2103 - acc: 0.9129\n",
      "Epoch 15/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2009 - acc: 0.9166\n",
      "Epoch 16/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2019 - acc: 0.9132\n",
      "Epoch 17/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1942 - acc: 0.9190\n",
      "Epoch 18/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1948 - acc: 0.9204\n",
      "Epoch 19/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1869 - acc: 0.9211\n",
      "Epoch 20/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.2044 - acc: 0.9189\n",
      "Epoch 21/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.2537 - acc: 0.9010\n",
      "Epoch 22/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.2473 - acc: 0.9022\n",
      "Epoch 23/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.1947 - acc: 0.9213\n",
      "Epoch 24/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.1982 - acc: 0.9214\n",
      "Epoch 25/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1691 - acc: 0.9274\n",
      "Epoch 26/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1807 - acc: 0.9250\n",
      "Epoch 27/50\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1750 - acc: 0.9260\n",
      "Epoch 28/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1700 - acc: 0.9263\n",
      "Epoch 29/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1640 - acc: 0.9313\n",
      "Epoch 30/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1797 - acc: 0.9245\n",
      "Epoch 31/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1780 - acc: 0.9271\n",
      "Epoch 32/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1775 - acc: 0.9279\n",
      "Epoch 33/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1687 - acc: 0.9285\n",
      "Epoch 34/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1837 - acc: 0.9235\n",
      "Epoch 35/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1763 - acc: 0.9288\n",
      "Epoch 36/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1756 - acc: 0.9288\n",
      "Epoch 37/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1783 - acc: 0.9274\n",
      "Epoch 38/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1680 - acc: 0.9325\n",
      "Epoch 39/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1674 - acc: 0.9300\n",
      "Epoch 40/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1716 - acc: 0.9268\n",
      "Epoch 41/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1643 - acc: 0.9330\n",
      "Epoch 42/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1723 - acc: 0.9311\n",
      "Epoch 43/50\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.1525 - acc: 0.9381\n",
      "Epoch 44/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.1742 - acc: 0.9291\n",
      "Epoch 45/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1652 - acc: 0.9326\n",
      "Epoch 46/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1585 - acc: 0.9336\n",
      "Epoch 47/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1650 - acc: 0.9317\n",
      "Epoch 48/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.1565 - acc: 0.9365\n",
      "Epoch 49/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.1593 - acc: 0.9308\n",
      "Epoch 50/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.1571 - acc: 0.9366\n",
      "1761/1761 [==============================] - 3s 2ms/step\n",
      "Loss = 0.3282805517456872\n",
      "Test Accuracy = 0.8637137989778535\n"
     ]
    }
   ],
   "source": [
    "#CONTINUED FROM CELL BELOW!!!\n",
    "#20% dropout at the end of every ident and conv block\n",
    "#model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "#adam = optimizers.Adam(0.0001)#, 0.9, 0.999, False)\n",
    "#model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12337/12337 [==============================] - 123s 10ms/step - loss: 1.6230 - acc: 0.3461\n",
      "Epoch 2/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 1.3821 - acc: 0.3976\n",
      "Epoch 3/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.2252 - acc: 0.4637\n",
      "Epoch 4/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.1350 - acc: 0.4920\n",
      "Epoch 5/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.0317 - acc: 0.5297\n",
      "Epoch 6/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.9532 - acc: 0.5761\n",
      "Epoch 7/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.8805 - acc: 0.6225\n",
      "Epoch 8/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.8067 - acc: 0.6703\n",
      "Epoch 9/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.7207 - acc: 0.7024\n",
      "Epoch 10/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.6774 - acc: 0.7139\n",
      "Epoch 11/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.6620 - acc: 0.7295\n",
      "Epoch 12/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.5981 - acc: 0.7480\n",
      "Epoch 13/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.6067 - acc: 0.7499\n",
      "Epoch 14/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.5555 - acc: 0.7711\n",
      "Epoch 15/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.5125 - acc: 0.7891\n",
      "Epoch 16/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4903 - acc: 0.7931\n",
      "Epoch 17/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4887 - acc: 0.7945\n",
      "Epoch 18/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4598 - acc: 0.8097\n",
      "Epoch 19/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4345 - acc: 0.8193\n",
      "Epoch 20/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4946 - acc: 0.8015\n",
      "Epoch 21/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4557 - acc: 0.8082\n",
      "Epoch 22/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.4107 - acc: 0.8312\n",
      "Epoch 23/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.3988 - acc: 0.8287\n",
      "Epoch 24/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.4013 - acc: 0.8321\n",
      "Epoch 25/50\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.3965 - acc: 0.8367\n",
      "Epoch 26/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.3708 - acc: 0.8451\n",
      "Epoch 27/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.3542 - acc: 0.8512\n",
      "Epoch 28/50\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.3606 - acc: 0.8449\n",
      "Epoch 29/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.3548 - acc: 0.8487\n",
      "Epoch 30/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.3396 - acc: 0.8580\n",
      "Epoch 31/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.4675 - acc: 0.8133\n",
      "Epoch 32/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.3525 - acc: 0.8478\n",
      "Epoch 33/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.4471 - acc: 0.8192\n",
      "Epoch 34/50\n",
      "12337/12337 [==============================] - 113s 9ms/step - loss: 0.3383 - acc: 0.8601\n",
      "Epoch 35/50\n",
      "12337/12337 [==============================] - 114s 9ms/step - loss: 0.3370 - acc: 0.8635\n",
      "Epoch 36/50\n",
      "12337/12337 [==============================] - 114s 9ms/step - loss: 0.3080 - acc: 0.8731\n",
      "Epoch 37/50\n",
      "12337/12337 [==============================] - 111s 9ms/step - loss: 0.3414 - acc: 0.8547\n",
      "Epoch 38/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.3126 - acc: 0.8641\n",
      "Epoch 39/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.3056 - acc: 0.8748\n",
      "Epoch 40/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2926 - acc: 0.8782\n",
      "Epoch 41/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2882 - acc: 0.8804\n",
      "Epoch 42/50\n",
      "12337/12337 [==============================] - 111s 9ms/step - loss: 0.2814 - acc: 0.8812\n",
      "Epoch 43/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2816 - acc: 0.8815\n",
      "Epoch 44/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2800 - acc: 0.8834\n",
      "Epoch 45/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2826 - acc: 0.8830\n",
      "Epoch 46/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2594 - acc: 0.8920\n",
      "Epoch 47/50\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.2636 - acc: 0.8908\n",
      "Epoch 48/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2646 - acc: 0.8907\n",
      "Epoch 49/50\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.2581 - acc: 0.8912\n",
      "Epoch 50/50\n",
      "12337/12337 [==============================] - 108s 9ms/step - loss: 0.2570 - acc: 0.8937\n",
      "1761/1761 [==============================] - 6s 4ms/step\n",
      "Loss = 0.5270920572186303\n",
      "Test Accuracy = 0.807495741056218\n"
     ]
    }
   ],
   "source": [
    "#20% dropout at the end of every ident and conv block\n",
    "#model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "#adam = optimizers.Adam(0.0001)#, 0.9, 0.999, False)\n",
    "#model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12337/12337 [==============================] - 125s 10ms/step - loss: 1.6052 - acc: 0.3460\n",
      "Epoch 2/10\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 1.4151 - acc: 0.4006\n",
      "Epoch 3/10\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 1.2328 - acc: 0.4641\n",
      "Epoch 4/10\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 1.1140 - acc: 0.5038\n",
      "Epoch 5/10\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.9923 - acc: 0.5466\n",
      "Epoch 6/10\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.9541 - acc: 0.5715\n",
      "Epoch 7/10\n",
      "12337/12337 [==============================] - 109s 9ms/step - loss: 0.8930 - acc: 0.6097\n",
      "Epoch 8/10\n",
      "12337/12337 [==============================] - 110s 9ms/step - loss: 0.7892 - acc: 0.6631\n",
      "Epoch 9/10\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 0.7486 - acc: 0.6834\n",
      "Epoch 10/10\n",
      "12337/12337 [==============================] - 106s 9ms/step - loss: 0.6756 - acc: 0.7182\n",
      "1761/1761 [==============================] - 6s 3ms/step\n",
      "Loss = 4.5605143016878005\n",
      "Test Accuracy = 0.21862578080636003\n"
     ]
    }
   ],
   "source": [
    "#dropout after every activation layer in ident and conv blocks\n",
    "model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "adam = optimizers.Adam(0.0001)#, 0.9, 0.999, False)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12337/12337 [==============================] - 107s 9ms/step - loss: 1.7078 - acc: 0.3263\n",
      "Epoch 2/10\n",
      "12337/12337 [==============================] - 94s 8ms/step - loss: 1.5943 - acc: 0.3280\n",
      "Epoch 3/10\n",
      "12337/12337 [==============================] - 94s 8ms/step - loss: 1.5600 - acc: 0.3280\n",
      "Epoch 4/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5453 - acc: 0.3280\n",
      "Epoch 5/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5381 - acc: 0.3280\n",
      "Epoch 6/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5341 - acc: 0.3280\n",
      "Epoch 7/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5317 - acc: 0.3280\n",
      "Epoch 8/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5307 - acc: 0.3280\n",
      "Epoch 9/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5296 - acc: 0.3280\n",
      "Epoch 10/10\n",
      "12337/12337 [==============================] - 93s 8ms/step - loss: 1.5290 - acc: 0.3280\n",
      "1761/1761 [==============================] - 5s 3ms/step\n",
      "Loss = 1.5276660708660839\n",
      "Test Accuracy = 0.3282226007950028\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "adam = optimizers.Adam(0.0001, 0.8, 0.999, False)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12337/12337 [==============================] - 112s 9ms/step - loss: 1.1836 - acc: 0.5345\n",
      "Epoch 2/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.8883 - acc: 0.6425\n",
      "Epoch 3/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.7298 - acc: 0.6981\n",
      "Epoch 4/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.6228 - acc: 0.7335\n",
      "Epoch 5/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.5630 - acc: 0.7673\n",
      "Epoch 6/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.5274 - acc: 0.7744\n",
      "Epoch 7/10\n",
      "12337/12337 [==============================] - 97s 8ms/step - loss: 0.4809 - acc: 0.7945\n",
      "Epoch 8/10\n",
      "12337/12337 [==============================] - 97s 8ms/step - loss: 0.4692 - acc: 0.8042\n",
      "Epoch 9/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.4664 - acc: 0.8098\n",
      "Epoch 10/10\n",
      "12337/12337 [==============================] - 96s 8ms/step - loss: 0.4179 - acc: 0.8204\n",
      "1761/1761 [==============================] - 6s 4ms/step\n",
      "Loss = 1.6051093418952893\n",
      "Test Accuracy = 0.5019875070982396\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "adam = optimizers.Adam(0.0001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=32)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12337/12337 [==============================] - 80s 7ms/step - loss: 1.2351 - acc: 0.5219\n",
      "Epoch 2/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.0402 - acc: 0.5597\n",
      "Epoch 3/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 0.7960 - acc: 0.6622\n",
      "Epoch 4/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 0.6683 - acc: 0.7155\n",
      "Epoch 5/20\n",
      "12337/12337 [==============================] - 64s 5ms/step - loss: 0.7726 - acc: 0.6950\n",
      "Epoch 6/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.6233 - acc: 0.4215\n",
      "Epoch 7/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.7098 - acc: 0.3550\n",
      "Epoch 8/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.4800 - acc: 0.3853\n",
      "Epoch 9/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.2667 - acc: 0.4784\n",
      "Epoch 10/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.3149 - acc: 0.4901\n",
      "Epoch 11/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.4465 - acc: 0.4202\n",
      "Epoch 12/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.2846 - acc: 0.4084\n",
      "Epoch 13/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.1896 - acc: 0.4654\n",
      "Epoch 14/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.1233 - acc: 0.4940\n",
      "Epoch 15/20\n",
      "12337/12337 [==============================] - 66s 5ms/step - loss: 1.3410 - acc: 0.4401\n",
      "Epoch 16/20\n",
      "12337/12337 [==============================] - 66s 5ms/step - loss: 1.2468 - acc: 0.4522\n",
      "Epoch 17/20\n",
      "12337/12337 [==============================] - 66s 5ms/step - loss: 1.1208 - acc: 0.4986\n",
      "Epoch 18/20\n",
      "12337/12337 [==============================] - 66s 5ms/step - loss: 1.0758 - acc: 0.5081\n",
      "Epoch 19/20\n",
      "12337/12337 [==============================] - 65s 5ms/step - loss: 1.0017 - acc: 0.5618\n",
      "Epoch 20/20\n",
      "12337/12337 [==============================] - 66s 5ms/step - loss: 0.9581 - acc: 0.5790\n",
      "1761/1761 [==============================] - 5s 3ms/step\n",
      "Loss = 1.2707267689610122\n",
      "Test Accuracy = 0.4997160704145372\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape=(IMSIZE,IMSIZE,3), classes = 6)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=20, batch_size=64)\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ' + str(preds[0]))\n",
    "print('Test Accuracy = ' + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
